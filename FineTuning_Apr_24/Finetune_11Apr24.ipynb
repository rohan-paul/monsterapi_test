{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune a Large Language Model (LLM) and deploy it on MonsterAPI ðŸ”¥\n",
    "\n",
    "The best part - costs less than a cup of coffee and no coding is required! ðŸ”¥\n",
    "\n",
    "-----\n",
    "\n",
    "@monsterapis designed their no-code LLM fine-tuner that simplifies the process of finetuning by:\n",
    "\n",
    "ðŸ‘‰ Automatically configuring GPU computing environments, optimised for higher throughput (with vllm in the backend).\n",
    "\n",
    "ðŸ‘‰ Optimizes memory usage by finding the optimal batch size,\n",
    "\n",
    "ðŸ‘‰ Integrates experiment tracking with WandB, and automatically pushing to Huggingface hub, and\n",
    "\n",
    "ðŸ‘‰ Auto configures the pipeline to complete without any errors on their cost-optimised GPU cloud\n",
    "\n",
    "ðŸ“Œ And all the above steps are without writing a single line of code.\n",
    "\n",
    "![](assets/2024-01-18-22-27-14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“Œ Recently Monsterapi finetuned google's Gemma-2B base model and the finetuned model outperforms LLaMA 13B on Mathematics reasoning.\n",
    "\n",
    "For this project, Gemma-2B underwent fine-tuning on the Microsoft/Orca-Math-Word-Problems-200K dataset for 10 epochs using MonsterAPI's MonsterTuner service.\n",
    "\n",
    "Evaluation was done on the GSM Plus benchmark, which is a specialized benchmark for checking mathematical proficiency of LLMs on grade-school mathematics\n",
    "\n",
    "ðŸ“Œ And MonsterAPI-finetuned Gemma-2B model achieved a remarkable score of 20.02 on this benchmark, representing a 68% improvement over its baseline model performance.\n",
    "\n",
    "And also this 2bn param finetuned model outperformed much larger models like LLaMA-2-13B and Code-LLaMA-7B.\n",
    "\n",
    "![](assets/2024-04-11-16-53-30.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual process for Finetuning an LLM\n",
    "\n",
    "ðŸ“Œ As a first step to finetune any model, Sign up for a Monster API account (monsterapi.ai/login) and get 2500 free credits.\n",
    "\n",
    "ðŸ“Œ Launch a Finetuning Portal, and choose from the latest Large Language Models (LLMs) such as Llama-2 7B, CodeLlama, Falcon or Mixtral 8X7B.\n",
    "\n",
    "ðŸ“Œ Dataset Preparation: You can choose from the curated selection of mostly used hugging face datasets with predefined training prompt configuration. OR\n",
    "\n",
    "You can use your own custom datasets, and we get a good amount of control around how the Dataset needs to be prepared in the right format. The portal provides a text-area in which target columns can be specified. Depending on the type of task chosen, you may have to alter the column names.\n",
    "\n",
    "ðŸ“Œ Specify Hyperparameter Configuration: such as epochs, learning rate, cutoff length, warmup steps, and so on.\n",
    "\n",
    "ðŸ“Œ Track stages of your finetuning jobs: Like, view job logs, monitor your job metrics using Weights & Biases. And finally upload model outputs to Huggingface.\n",
    "\n",
    "------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ðŸ“Œ Once you have finetuned an LLM on MonsterAPI, you will receive adapter weights as the final output. This adapter contains your fine-tuned modelâ€™s weights that Monster will host as an API endpoint using Monster Deploy.\n",
    "\n",
    "ðŸ“Œ MonsterDeploy optimizes its backend operations using vLLM framework. vLLM is a rapid and user-friendly library for large language model inference and serving, notable for its state-of-the-art serving throughput.\n",
    "\n",
    "And MonsterAPI also hosts wide range of popular LLMs as inference service and you can use popular tools like llama-index to access MonsterAPI LLMs. Indeed, Monster Deploy enables you to host any vLLM supported large language model (LLM) like Tinyllama, Mixtral, Phi-2 etc as a rest API endpoint on MonsterAPI's cost optimised GPU cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example code in image, deploys the Mixtral 8x7b Chat model with GPTQ 4bit quantization by using a 48GB GPU, using Monster Deploy.\n",
    "\n",
    "The Deployment will be able to serve the model as a REST API for both static and streaming token response support.\n",
    "\n",
    "```py\n",
    "!python3 -m pip install monsterapi==1.0.2b3\n",
    "# install specific beta version of client for quick serve access.\n",
    "\n",
    "api_key = \"YOUR_MONSTER_API_KEY\"\n",
    "from monsterapi import client as mclient\n",
    "deploy_client = mclient(api_key = api_key)\n",
    "\n",
    "# deploy Mixtral 8x7b Chat model with GPTQ 4bit quantization\n",
    "# using a 48GB GPU.\n",
    "basemodel_path=\"TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\"\n",
    "    prompt_template=\"<s> [INST] {instruction} [/INST] {completion}</s>\"\n",
    "    api_auth_token=\"A_RANDOM_AUTH_TOKEN_TO_SECURE_YOUR_ENDPOINT\"\n",
    "    per_gpu_vram=48\n",
    "    gpu_count=1\n",
    "\n",
    "# Launch a deployment\n",
    "launch_payload = {\n",
    "    \"basemodel_path\": \"TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\",\n",
    "    \"prompt_template\": \"<s> [INST] {prompt} [/INST] {completion}</s>\",\n",
    "    \"api_auth_token\": \"b6a97d3b-35d0-4720-a44c-59ee33dbc25b\",\n",
    "    \"per_gpu_vram\": 48,\n",
    "    \"gpu_count\": 1,\n",
    "    \"use_nightly\": True\n",
    "}\n",
    "\n",
    "# Launch a deployment\n",
    "ret = deploy_client.deploy(\"llm\", launch_payload)\n",
    "deployment_id = ret.get(\"deployment_id\")\n",
    "print(deployment_id)\n",
    "\"\"\"\n",
    "{\n",
    "     \"status\":\"live\",\n",
    "     \"message\":\"Server has started !!!\",\n",
    "     \"URL\":\"https://c503a813-850a-4a78-93b9.monsterapi.ai\",\n",
    "     \"api_auth_token\":\"57b7b903-a4b6-4720-8154-af71aa8e8313\"\n",
    " }\n",
    " visit the url to get the llm service endpoint details\n",
    " or above url/docs to get swagger docs\n",
    "\"\"\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the deployment is live, let's query our deployed LLM endpoint:\n",
    "\n",
    "```py\n",
    "import json\n",
    "\n",
    "status_debug = True # Just a placeholder to show possible statuses.\n",
    "\n",
    "if status_debug:\n",
    "  status_ret = deploy_client.get_deployment_status(deployment_id)\n",
    "  print(status_ret)\n",
    "\n",
    "assert status_ret.get(\"status\") == \"live\", \"Please wait until status is live!\"\n",
    "\n",
    "service_client  = mclient(api_key = status_ret.get(\"api_auth_token\"),\n",
    "                          base_url = status_ret.get(\"URL\"))\n",
    "\n",
    "payload = {\n",
    "    \"input_variables\": {\n",
    "        \"prompt\": \"What's up?\"},\n",
    "    \"stream\": False,\n",
    "    \"temperature\": 0.6,\n",
    "    \"max_tokens\": 2048\n",
    "}\n",
    "\n",
    "output = service_client.generate(model = \"deploy-llm\", data = payload)\n",
    "\n",
    "if payload.get(\"stream\"):\n",
    "    for i in output:\n",
    "        print(i[0])\n",
    "else:\n",
    "    print(json.loads(output)['text'][0])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a wrap and here are all the important links.\n",
    "\n",
    "ðŸ‘‰ Website : http://monsterapi.ai\n",
    "\n",
    "ðŸ‘‰ Discord (Monsterapis) : https://discord.com/invite/mVXfag4kZN\n",
    "\n",
    "ðŸ‘‰ Checkout their API Docs - https://developer.monsterapi.ai/docs/monster-deploy-beta\n",
    "\n",
    "ðŸ‘‰ Access all Finetuned Models by Monster here:\n",
    "\n",
    "https://huggingface.co/qblocks?ref=blog.monsterapi.ai"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
